## CHAPTER 15 공간 기반 아키텍처 스타일

```
인상깊은 내용)
이 아키텍처는 명확한 문제에 대한 해결책을 갖고 있는 아키텍처 스타일이라는 점에서 
책에서 설명한 것과 같이 확장성, 동시성 등에 문제를 해결해야 하는 도메인이라면
아무리 마이크로서비스+도메인 주도 아키텍처가 끝판왕이라 해도
이 아키텍처를 고려해볼 것 같긴 합니다.

특히 인메모리 복제 DB, CQRS를 생각하게 하는 data reader/writer의 구성은
확실히 아키텍처 기초중에 하나를 잘 배우고 간다는 느낌이 많이 들었습니다.
```

유저 수 증가에 따른 병목 현상의 일반적인 해결 방법으로 웹 서버 확장이 있는데, 유저 부하가 높을 때 웹 서버 레이어를 확장하면 병목점은 다시 애플리케이션 서버로 이동하기 쉽다. 애플리케이션 서버를 확장하더라도 다시 데이터베이스 서버가 병목점이 될 가능성이 높다. 따라서 아래 그림처럼 삼각형 토폴로지가 그러지는데 길이가 긴 변이 웹 서버, 길이가 짧은 변이 데이터베이스가 된다.

![image](https://github.com/jongfeel/BookReview/assets/17442457/47da3124-a705-481f-b93f-5de41c8241c0)

공간 기반 아키텍처 스타일space-based architeccture style은 높은 확장성, 탄력성, 동시성 및 이와 관련된 문제를 해결하기 위해 설계된 아키텍처 스타일이다. 동시 유저 수가 가변적이어서 예측하기 어려운 애플리케이션에 유용하다. 데이터베이스 확장이나 확장성이 떨어지는 아키텍처의 캐시 기술을 적용하는 것 보다 이 아키텍처로 해결하는 것이 더 낫다.

### 15.1 토폴로지

공간 기반 아키텍처라는 명칭은 튜플 공간tuple space에서 유래됐다. 튜플 공간은 공유 메모리를 통해 통신하는 다중 병렬 프로세서를 사용하는 기술이다. 시스템에서 동기 제약조건인 중앙 데이터베이스를 없애는 대신, 복제된 인메모리 데이터 그리드in-memory data grid를 활용하면 확장성, 탄력성, 성능을 높일 수 있다. 애플리케이션 데이터는 메모리에 둔 상태로 모든 활성 처리 장치들이 데이터를 복제한다. 처리 장치는 데이터를 업데이트 할 떄 퍼시스턴스 큐persistent queue에 메시지를 보내는 식으로 데이터베이스에 데이터를 비동기 전송한다. 처리 장치는 동적으로 시작/종료가 가능하므로 가변적으로 확장할 수 있다. 중앙 데이터베이스가 애플리케이션의 표준 트랜잭션에 관여하지 않으므로 데이터베이스 병목 현상이 사라지고 애플리케이션은 거의 무한에 가까운 확장성이 보장된다.

![image](https://github.com/jongfeel/BookReview/assets/17442457/f7600dd3-ddab-4141-9eb4-0e0b45b2d1dc)

#### 15.1.1 처리 장치

처리 장치processing unit는 애플리케이션 로직을 갖고 있다. 보통 웹 기반 컴포넌트와 백엔드 비즈니스 로직이 포함된다. 작은 웹 애플리케이션은 단일 처리 장치에 배포 가능하지만, 대규모 애플리케이션은 기능별로 여러 처리 장치에 나눠 배포한다. 애플리케이션 로직 외에도 헤이즐캐스트Hazelcast, 아파치 이그나이트Apache Ignite, 오라클 코히어런스Oracle Coherence 등의 제품에 있는 인메모리 데이터 그리드 및 복제 엔진도 처리 장치에 포함된다.

![image](https://github.com/jongfeel/BookReview/assets/17442457/0748ec46-d758-4cf4-bd37-a1a34904d12b)

#### 15.1.2 가상 미들웨어

가상 미들웨어virtualized middleware는 아키텍처 내부에서 데이터 동기화 및 요청 처리의 다양한 부분을 제어하는 인프라를 담당한다. 가상 미들웨어는 메시징 그리드messaging grid, 데이터 그리드data grid, 처리 그리드processing grid, 배포 관리자deployment manager 등의 컴포넌트로 구성된다.

**메시징 그리드**

메시징 그리드는 입력 요청과 세션 상태를 관리한다. 가상 미들웨어에 요청이 유입되면 메시징 그리드는 어느 활성 처리 장치가 요청을 받아 처리할지 결정하여 해당 처리 장치로 요청을 전달한다. 이 컴포넌트는 부하 분산이 가능한 일반 웹 서버로 구현한다.

![image](https://github.com/jongfeel/BookReview/assets/17442457/b415686d-681f-4da2-925a-9e457378c8db)

**데이터 그리드**

데이터 그리드data grid는 이 아키텍처 스타일에서 가장 중요하고 필수적인 컴포넌트이다. 외부 컨트롤러가 필요한 복제 캐시 구현체나 분산 캐시distributed cache를 사용할 경우, 데이터 그리드는 가상 미들웨어 내부의 데이터 그리드 컴포넌트와 처리 장치 모두에 위치한다. 각 처리장치는 자신의 인메모리 데이터 그리드에 정확히 동일한 데이터를 갖고 있어야 한다. 실제 데이터 동기화는 비동기 방식으로 신속하게 100ms 미만으로 이루어진다.

![image](https://github.com/jongfeel/BookReview/assets/17442457/b61320b8-0191-4794-b91b-78d994c19d85)

데이터는 처리 장치 내부에서 복제되므로 데이터베이스에서 데이터를 읽지 않아도 서비스 인스턴스의 기동/중지가 가능하다. 단, 이름을 부여한named 복제 캐시를 가진 인스턴스가 하나 이상 필요하다.

각 처리 장치는 멤버 리스트를 사용해 다른 모든 처리 장치 인스턴스를 인지한다. 멤버 리스트에는 동일한 이름의 캐시를 사용하는 다른 모든 처리 장치의 IP 주소 및 포트가 들어 있다.

데이터 그리드에서는 헤이즐캐스트로 인스턴스 멤버 리스트를 관리하는 예제가 있다.
아래 주소는 헤이즐캐스트 github 주소이다.
https://github.com/hazelcast/hazelcast

**처리 그리드**

처리 그리드processing grid는 가상 미들웨어에서 다수의 처리 장치가 단일 비즈니스 요청을 처리할 경우 요청 처리를 오케스트레이트 하는 일을 한다. 또 종류가 다른 처리 장치 사이에 조정이 필요한 요청이 들어오면 처리 그리드가 두 처리 장치 사이에서 요청을 중재/조정한다.

![image](https://github.com/jongfeel/BookReview/assets/17442457/0d3cce60-f08a-4bed-a02c-29134d9ec958)

**배포 관리자**

배포 관리자deployment manager는 부하 조건에 따라 처리 장치 인스턴스를 동적으로 시작/종료하는 컴포넌트이다. 응답 시간, 유저 부하를 계속 모니터링 하면서 부하가 증가하면 새로운 처리 장치를 기동하고 반대로 감소하면 기존 처리 장치를 종료한다. 다양한 확장성(탄력성) 요구사항을 구현하는데 필요한 컴포넌트이다.

#### 15.1.3 데이터 펌프

데이터 펌프data pump는 데이터를 다른 프로세서에 보내 데이터베이스를 업데이트하는 장치이다. 처리 장치가 데이터베이스를 직접 액세스 하지 않으므로 데이터 펌프가 필요하다. 비동기로 동작하며 메모리 캐시와 데이터베이스의 최종 일관성을 유지한다.

데이터 펌프는 메시징 기법으로 구현한다. 메시징은 비동기 통신을 지원하고 전달을 보장하며 FIFO 큐를 통해 메시지 순서도 유지한다. 메시징을 이용하면 처리 장치와 데이터 라이터data writer를 분리할 수 있어서 데이터 라이터를 사용할 수 없는 경우에도 처리장치에서 무중단 처리가 가능하다.

![image](https://github.com/jongfeel/BookReview/assets/17442457/ef813da9-25d9-40e5-ae26-357b6e00eb9e)

데이터 펌프는 도메인이나 서브 도메인 별로 여러 개를 사용한다. 

데이터 펌프는 계약 데이터와 연관된 액션(CRUD)를 포함한다. 계약 포맷은 json, xml, object, 값 기반 메시지value-driven message 등 다양하다.

#### 15.1.4 데이터 라이터

데이터 라이터data writer는 데이터 펌프에서 메시지를 받아 그에 맞게 데이터베이스를 업데이트하는 컴포넌트이다. 서비스나 애플리케이션, 데이터 허브(예: 앱 이니셔Ab Initio)로 구현할 수 있다.

도메인 기반의 데이터 라이터는 데이터 펌프 수와 무관하며 특정 도메인의 전체 업데이트를 처리하는 데 필요한 모든 데이터베이스 로직을 갖고 있다. 고객 도메인의 경우 도메인 처리 장치 4개, 데이터 펌프 4개로 처리하지만 데이터 라이터는 하나로 배치할 수 있다.

![image](https://github.com/jongfeel/BookReview/assets/17442457/509d8406-afb3-415e-8b4c-b60a7ad7caf3)

처리 장치 클래스마다 자체 전용 데이터 라이터를 두는 경우도 있다. 이 모델은 데이터 라이터가 많은 단점이 있지만, 처리 장치, 데이터 펌프, 데이터 라이터가 나란히 정렬되어 확장성, 민첩성은 더 좋아진다.

![image](https://github.com/jongfeel/BookReview/assets/17442457/15a82139-758d-4b30-8d37-8aeb6e9c51c4)

#### 15.1.5 데이터 리더

데이터 리더data reader는 데이터베이스에서 데이터를 읽어 리버스 데이터 펌프reverse data pump를 통해 처리 장치로 실어 나르는 컴포넌트이다.

데이터 리더는 세 가지 경우에만 작동한다.

- 동일한 이름의 캐시를 가진 모든 처리 장치 인스턴스가 실패하는 경우
- 동일한 이름의 캐시 안에서 모든 처리 장치를 재배포하는 경우
- 복제 캐시에 들어있지 않은 아카이브 데이터를 조회하는 경우

데이터 리더도 데이터 라이터처럼 도메인 기반으로 할 수 있지만 특정 처리 장치의 클래스 전용으로 사용한다. 서비스, 애플리케이션, 데이터 허브 모두 구현체는 데이터 라이터와 동일하다.

데이터 라이터와 데이터 리더는 데이터 추상 레이어data abstraction layer(또는 데이터 액세스 레이어data access layer)를 형성한다. 두 레이어의 차이점은 처리 장치가 데이터베이스의 테이블 구조를 얼마나 자세히 알고 있는가 이다.

데이터 액세스 레이어는 데이터베이스와 커플링 되어 있으므로 데이터 리더/라이터만 사용해서 간접적으로 데이터베이스에 접근한다.

데이터 추상 레이어는 처리 장치가 데이터베이스와 분리되어 있다. 공간 기반 아키텍처는 데이터 추상 레이어 모델이 기반하므로 처리 장치의 복제 캐시 스키마는 데이터베이스의 테이블 구조와 다를 수 있어서, 처리 장치에 영향을 미치지 않고서도 데이터베이스 증분 변경incremental change이 가능하며, 데이터 리더/라이터에 이미 변환 로직이 포함되어 있기 때문에 이런 증분 변경이 더 용이하다.

### 15.2 데이터 충돌

이름이 동일한 캐시가 포함된 서비스 인스턴스에 active/active 상태에서 복제 캐시를 사용하면 복제 레이턴시replication latency 때문에 데이터 충돌data collision이 발생할 수 있다. 데이터 충돌은 한 캐시 인스턴스에서 데이터가 업데이트 되어 다른 캐시 인스턴스에 복제하는 도중에 동일한 데이터가 해당 캐시에서 업데이트 되는 현상을 말한다.

데이터 충볼 발생 빈도는 동일한 캐시를 포함한 처리 장치 인스턴스 수, 캐시 업데이트율, 캐시 크기, 캐시 제품의 복제 레이턴시 등 여러 팩터가 영향을 미친다.

가변적인 복제 레이턴시는 데이터 일관성에 중대한 영향을 미친다. 네트워크 유형, 처리 장치 간 물리적 거리 등 다양한 팩터에 좌우되기 때문에 복제 레이턴시 값이 공시되는 경우는 거의 없어서 직접 프로덕션 환경에서 값을 측정해야 한다.

충돌률을 계산할 때에는 사용량이 가장 많은 시점의 최대 업데이트율에 따라 최소, 정상, 최대 충돌률을 산출하는 것이 바람직하다.

### 15.3 클라우드 대 온프레미스 구현

공간 기반 아키텍처는 배포 환경에서 선택지가 있다. 전체 토폴로지는 클라우드 기반의 환경이나 온프레미스에 배포할 수 있다. 하지만 두 환경 사이에 어중간하게 배포할 수도 있는데, 이것이 다른 아키텍처 스타일에서는 찾아볼 수 없는 이 아키텍처의 특징이다.

물리 데이터베이스와 데이터는 온프레미스에, 클라우드 기반의 매니지드managed 환경에서 처리 장치와 가상 미들웨어를 통해 애플리케이션을 배포하는 하이브리드 클라우드 구성이 가능하다.

![image](https://github.com/jongfeel/BookReview/assets/17442457/5938de97-aa51-4fdd-a801-47be45a204b3)

트랜잭션은 탄력적인 동적 클라우드 기반의 환경에서 처리하되, 물리적인 데이터 관리, 리포팅, 데이터 분석 데이터는 안전한 로컬 온프레미스 환경에 보관할 수 있다.

### 15.4 복제 캐시 대 분산 캐시

복제 캐시를 사용할 경우, 각 처리 장치는 이름이 같은 캐시를 사용하는 모든 처리 장치 간에 동기화되는 자체 인메모리 데이터 그리드를 갖고 있다. 한 처리 장치에서 캐시가 업데이트 되면 다른 처리 장치도 새로운 데이터로 자동 업데이트되는 구조이다.

<img width="470" alt="image" src="https://github.com/jongfeel/BookReview/assets/17442457/83cf6d40-7350-4fd7-af53-7185e47b3c79">

복제 캐시는 속도가 매우 빠르고 높은 수준의 내고장성을 지원하며 중앙 서버에서 캐시를 갖고 있는 형태가 아니므로 단일 장애점이 없다

복제 캐시는 공간 기반 아키텍처의 표준 캐시 모델이지만, 데이터량이 많거나 캐시 데이터가 너무 빈번하게 업데이트 되는 일이 생기면 복제 캐시를 사용할 수 없는 경우도 있다.

캐시 데이터 업데이트율이 높은 경우 처리 장치 인스턴스에서 업데이트가 빠르게 이루어져야 하지만
데이터 그리드가 미처 이 속도를 따라가지 못할 수도 있다. 이럴 때는 분산 캐시를 사용하면 도움이 된다.

분산 캐시는 중앙 캐시를 갖고 있는 전용 외부 서버나 서비스가 필요하다. 모든 데이터가 한 곳에 있고 복제할 필요가 없어 분산 캐시는 높은 수준의 데이터 일관성을 보장하지만, 캐시 데이터를 원격에서 가져와야 하므로 복제 캐시보다 성능이 낮고 시스템 전체 레이턴시가 증가한다. 분산 캐시를 미러링하면 어느 정도 내고장성을 확보할 수 있지만, 미러링된 캐시 서버로 제때 데이터가 이동하지 못하면 일관성에 문제가 생기게 된다.

<img width="472" alt="image" src="https://github.com/jongfeel/BookReview/assets/17442457/1318b450-3319-4fd1-805e-b560ad55dc20">

캐시가 작고 업데이트율이 낮아서 캐시 제품의 복제 엔진에 캐시 업데이트를 충분히 따라올 수 있다면
복제 캐시냐 분산 캐시냐의 선택은 데이터의 일관성이냐, 성능/내고장성이냐의 문제로 귀결된다.

일관성이 중요한 데이터는 분산 캐시를 (예: 제품 재고)
자주 변경되지 않는 데이터는 복제 캐시를 사용한다. (예: 제품 코드, 제품 설명)

<img width="505" alt="image" src="https://github.com/jongfeel/BookReview/assets/17442457/7838ac40-4c88-4604-b23e-410d0c2c4f22">

캐시 모델 선정에는 주어진 애플리케이션 콘텍스트에서 두 모델 적용 가능하다는 점을 기억해야 한다. 즉 한 모델로 모든 문제를 해결할 수는 없다.

### 15.5 니어 캐시

니어 캐시near-cache는 분산 캐시와 인메모리 데이터 그리드를 접합한 일종의 하이브리드 캐시 모델이다.
이 모델에서 분산 캐시는 풀 백킹 캐시full backing cache
각 처리 장치에 포함된 인메모리 데이터 그리드는 프런트 캐시front cache라고 한다.
프런트 캐시는 가장 최근에 사용한 항목이 포함된 MRUMost Recently Used 캐시 또는
가장 자주 사용한 항목이 포함되는 MFUMost Frequently Used 캐시로 사용한다.

<img width="468" alt="image" src="https://github.com/jongfeel/BookReview/assets/17442457/371fc97d-7142-467d-9232-1f903ba12fd6">

프런트 캐시는 항상 풀 백킹 캐시와 동기화되지만 각 처리 장치에 포함된 프런트 캐시는 동일한 데이터를공유하는 다른 처리 장치와 동기화되지 않는다.

처리 장치마다 상이한 데이터를 프런트 캐시에 갖게 되고 처리 장치 간 성능과 응답성의 일관성이 결여되므로,
공간 기반 아키텍처에서 니어 캐시 모델은 권장하지 않는다. 

### 15.6 구현 예시

유저 수나 요청량이 갑자기 폭증하는 애플리케이션 혹은 10000명이 넘는 동시 유저를 처리해야 하는 종류의 애플리케이션에 적합하다.

#### 15.6.1 콘서트 티켓 판매 시스템

평소에는 유저 수가 적게 유지되다가 인기있는 콘서트 티켓 발매가 시작되면 티켓을 사려는 동시 유저 수가 수백~수천 단위로 급증하는 패턴이 있다. 따라서 공간 기반 아키텍처에서 지원하는 아키텍처 특성이 필요하다.

공간 기반 아키텍처는 고도의 탄력성을 제공하므로 이런 시스템에 알맞다. 콘서트 티켓을 구매하는 동시 유저 수가 순간적으로 치솟으면 배포 관리자는 이 사실을 바로 인지하여 대량 요청을 감당할 수 있도록 다수의 처리 장치를 기동한다. 

#### 15.6.2 온라인 경매 시스템

온라인 경매 시스템 역시 고도의 성능과 탄력성이 필요하고, 유저 및 요청 부하가 예기치 않게 폭증하는 특징이 있다.

따라서 부하가 증가하면 처리 장치를 여럿 기동할 수 있는 공간 기반 아키텍처가 이런 종류의 문제 영역에 적합하다.

### 15.7 아키텍처 특성 등급

<img width="473" alt="image" src="https://github.com/jongfeel/BookReview/assets/17442457/30053578-ab20-4089-8f05-88271367d3f5">

공간 기반 아키텍처는 탄력성, 확장성, 성능의 끝판왕이다.

전체적인 단순성과 시험성 측면에서는 트레이드오프가 있다. 주요 데이터 저장소에서 캐시를 사용하고 최종 일관성이라는 개념을 적용하기 때문에 구조가 복잡하다. 충돌이 발생해 데이터가 소실되는 일은 없도록 해야 한다.

수십만명의 동시 유저를 테스트 하는 일은 대단히 복잡하고 비용이 많이 드는 일이므로 시험성은 별점 1점이다.

비용 역시 고려해야 할 대상인데, 사용 캐시 제품을 사용하기 위해서 라이선시 비용을 지불해야 하고 리소스 사용률이 많으므로 상대적으로 돈이 많이 든다.

공간 기반 아키텍처의 분할 유형은 식별이 쉽지 않아서 도메인 + 기술 분할로 표시한다.
특정 유형의 도메인에 따라 조정되고 처리 장치도 유연하므로 도메인 분할이 된다. 처리 장치는 서비스 기반 아키텍처나 마이크로서비스 아키텍처에서 서비스가 정의되는 것과 같이 도메인 서비스로 작동 시킬 수 있다.
데이터 펌프를 통해 데이터베이스의 실제 데이터 저장소에서 캐시를 이용해 트랜잭션 처리 문제를 분리하는 방식으로 기술 분할 된다.